{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzTZSFew2RSJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip -q install torch torchvision torchaudio codecarbon pandas numpy scikit-learn >/dev/null 2>&1 || true\n",
        "\n",
        "import os, time, random, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"DEVICE:\", DEVICE)\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "y_train = y_train.squeeze().astype(np.int64)\n",
        "y_test  = y_test.squeeze().astype(np.int64)\n",
        "\n",
        "\n",
        "x_train = (x_train.astype(np.float32) / 255.0).transpose(0, 3, 1, 2)  # (N,3,32,32)\n",
        "x_test  = (x_test.astype(np.float32) / 255.0).transpose(0, 3, 1, 2)\n",
        "\n",
        "class CIFAR10NP(Dataset):\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, i):\n",
        "        return torch.from_numpy(self.X[i]), int(self.y[i])\n",
        "\n",
        "full_train = CIFAR10NP(x_train, y_train)\n",
        "full_test  = CIFAR10NP(x_test, y_test)\n",
        "\n",
        "\n",
        "def dirichlet_split_noniid_with_size(labels, n_clients=10, alpha=0.3, size_skew=True, min_size=50):\n",
        "    labels = np.array(labels)\n",
        "    n_classes = int(labels.max()) + 1\n",
        "    while True:\n",
        "        per_client = [[] for _ in range(n_clients)]\n",
        "        for c in range(n_classes):\n",
        "            idx = np.where(labels == c)[0]\n",
        "            np.random.shuffle(idx)\n",
        "            props = np.random.dirichlet([alpha] * n_clients)\n",
        "            if size_skew:\n",
        "                w = np.random.lognormal(mean=0.0, sigma=0.5, size=n_clients)\n",
        "                props = props * w\n",
        "                props = props / props.sum()\n",
        "            splits = (np.cumsum(props) * len(idx)).astype(int)[:-1]\n",
        "            chunks = np.split(idx, splits)\n",
        "            for i, ch in enumerate(chunks):\n",
        "                per_client[i].extend(ch.tolist())\n",
        "\n",
        "        ok = all(len(x) >= min_size for x in per_client)\n",
        "        if ok:\n",
        "            for i in range(n_clients):\n",
        "                random.shuffle(per_client[i])\n",
        "            return per_client\n",
        "\n",
        "client_indices = dirichlet_split_noniid_with_size(\n",
        "    labels=y_train, n_clients=10, alpha=0.3, size_skew=True, min_size=50\n",
        ")\n",
        "\n",
        "\n",
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "class CCTracker:\n",
        "    \"\"\"CodeCarbon tracker returning energy in Wh.\"\"\"\n",
        "    def __init__(self, name=\"section\"):\n",
        "        self.name = name\n",
        "        self.tracker = None\n",
        "    def start(self):\n",
        "\n",
        "        self.tracker = EmissionsTracker(project_name=self.name, measure_power_secs=1)\n",
        "        self.tracker.start()\n",
        "    def stop_wh(self) -> float:\n",
        "        try:\n",
        "            self.tracker.stop()\n",
        "        finally:\n",
        "            pass\n",
        "        kwh = 0.0\n",
        "\n",
        "        if hasattr(self.tracker, \"_total_energy\") and hasattr(self.tracker._total_energy, \"kWh\"):\n",
        "            kwh = float(self.tracker._total_energy.kWh)\n",
        "        elif hasattr(self.tracker, \"final_emissions_data\") and self.tracker.final_emissions_data:\n",
        "\n",
        "            kwh = float(self.tracker.final_emissions_data.energy_consumed or 0.0)\n",
        "        return 1000.0 * kwh\n",
        "\n",
        "\n",
        "# Homogeneous CNN search space\n",
        "\n",
        "class HomoCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN defined by (depth, width, kernel).\n",
        "    depth âˆˆ {2,3,4}, width âˆˆ {16,32,64}, kernel âˆˆ {3,5}\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10, depth=3, width=32, kernel=3):\n",
        "        super().__init__()\n",
        "        self._arch = {\"depth\": depth, \"width\": width, \"kernel\": kernel}\n",
        "        k = kernel\n",
        "        chs = [width, width*2, width*2, width*2][:depth]\n",
        "        layers = []\n",
        "        in_ch = 3\n",
        "        for i, out_ch in enumerate(chs):\n",
        "            layers += [\n",
        "                nn.Conv2d(in_ch, out_ch, k, padding=k//2, bias=False),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            if i < 2:\n",
        "                layers += [nn.MaxPool2d(2)]\n",
        "            in_ch = out_ch\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_ch, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.head(x)\n",
        "\n",
        "def build_model(arch: Dict):\n",
        "    m = HomoCNN(depth=arch[\"depth\"], width=arch[\"width\"], kernel=arch[\"kernel\"]).to(DEVICE)\n",
        "    m._arch = arch.copy()\n",
        "    return m\n",
        "\n",
        "def count_params(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def _conv2d_flops(conv: nn.Conv2d, out_h: int, out_w: int) -> int:\n",
        "    cin = conv.in_channels\n",
        "    cout = conv.out_channels\n",
        "    k_h, k_w = conv.kernel_size if isinstance(conv.kernel_size, tuple) else (conv.kernel_size, conv.kernel_size)\n",
        "    groups = conv.groups\n",
        "    muls_per_out = (cin // groups) * k_h * k_w\n",
        "    # multiply + add â‰ˆ 2*muls\n",
        "    return int(cout * out_h * out_w * (2 * muls_per_out))\n",
        "\n",
        "def _linear_flops(fc: nn.Linear) -> int:\n",
        "    return int(2 * fc.in_features * fc.out_features)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_homocnn_flops_per_sample(model: HomoCNN, input_hw=(32, 32)) -> int:\n",
        "    \"\"\"\n",
        "    Forward FLOPs per sample for HomoCNN (Conv+Linear only).\n",
        "    Ignores BN/ReLU/Pool FLOPs (small vs conv).\n",
        "    \"\"\"\n",
        "    H, W = input_hw\n",
        "    flops = 0\n",
        "\n",
        "    for layer in model.features:\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            flops += _conv2d_flops(layer, H, W)\n",
        "        elif isinstance(layer, nn.MaxPool2d):\n",
        "            H //= 2\n",
        "            W //= 2\n",
        "\n",
        "    for layer in model.head:\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            flops += _linear_flops(layer)\n",
        "\n",
        "    return int(flops)\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module, dataset: Dataset, batch_size=256):\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    model.eval(); correct=0; total=0\n",
        "    with torch.no_grad():\n",
        "        for xb,yb in loader:\n",
        "            xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            pred = model(xb).argmax(1)\n",
        "            correct += (pred==yb).sum().item(); total += yb.size(0)\n",
        "    return correct/total\n",
        "\n",
        "def evaluate_loader(model: nn.Module, loader: DataLoader):\n",
        "    model.eval(); correct=0; total=0\n",
        "    with torch.no_grad():\n",
        "        for xb,yb in loader:\n",
        "            xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            pred = model(xb).argmax(1)\n",
        "            correct += (pred==yb).sum().item(); total += yb.size(0)\n",
        "    return (correct/total) if total>0 else 0.0\n",
        "\n",
        "\n",
        "class Client:\n",
        "    def __init__(self, cid: int, indices: List[int], batch_size=32):\n",
        "        self.cid = cid\n",
        "        self.dataset = Subset(full_train, indices)\n",
        "        self.loader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "        self.eval_loader = DataLoader(self.dataset, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    def local_train(self, global_model: nn.Module, epochs=1, lr=1e-3, weight_decay=1e-4):\n",
        "\n",
        "        model = build_model(global_model._arch)\n",
        "        model.load_state_dict(global_model.state_dict(), strict=True)\n",
        "        model.train()\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "        cc = CCTracker(name=f\"client_{self.cid}\")\n",
        "        start_t = time.perf_counter()\n",
        "        cc.start()\n",
        "\n",
        "        for _ in range(epochs):\n",
        "            for xb,yb in self.loader:\n",
        "                xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "                opt.zero_grad()\n",
        "                logits = model(xb)\n",
        "                loss = F.cross_entropy(logits, yb)\n",
        "                loss.backward(); opt.step()\n",
        "\n",
        "        e_wh = cc.stop_wh()\n",
        "        end_t = time.perf_counter()\n",
        "        train_time_sec = float(end_t - start_t)\n",
        "\n",
        "\n",
        "        local_acc = evaluate_loader(model, self.eval_loader)\n",
        "\n",
        "\n",
        "        return {k: v.detach().cpu() for k,v in model.state_dict().items()}, len(self.dataset), e_wh, local_acc, train_time_sec\n",
        "\n",
        "\n",
        "def fedavg(global_model: nn.Module, client_payloads: List[Tuple[dict,int,float,float,float]]):\n",
        "    total = sum(n for _,n,_,_,_ in client_payloads)\n",
        "    new_sd = {}\n",
        "    keys = client_payloads[0][0].keys()\n",
        "    for k in keys:\n",
        "        new_sd[k] = sum(sd[k]*(n/total) for sd,n,_,_,_ in client_payloads)\n",
        "    global_model.load_state_dict(new_sd, strict=True)\n",
        "    return global_model\n",
        "\n",
        "\n",
        "class RLController(nn.Module):\n",
        "    \"\"\"\n",
        "    Controller over depth {2,3,4}, width {16,32,64}, kernel {3,5}\n",
        "    Each global epoch: sample an arch, run one FL round, update controller via reward.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.depth_logits  = nn.Parameter(torch.zeros(3))\n",
        "        self.width_logits  = nn.Parameter(torch.zeros(3))\n",
        "        self.kernel_logits = nn.Parameter(torch.zeros(2))\n",
        "\n",
        "    def sample_arch(self):\n",
        "        d_cat = torch.distributions.Categorical(logits=self.depth_logits)\n",
        "        w_cat = torch.distributions.Categorical(logits=self.width_logits)\n",
        "        k_cat = torch.distributions.Categorical(logits=self.kernel_logits)\n",
        "        d_i, w_i, k_i = d_cat.sample(), w_cat.sample(), k_cat.sample()\n",
        "        arch = {\"depth\": [2,3,4][int(d_i)],\n",
        "                \"width\": [16,32,64][int(w_i)],\n",
        "                \"kernel\": [3,5][int(k_i)]}\n",
        "        logp = d_cat.log_prob(d_i) + w_cat.log_prob(w_i) + k_cat.log_prob(k_i)\n",
        "        return arch, logp\n",
        "\n",
        "    def parameters_list(self):\n",
        "        return [self.depth_logits, self.width_logits, self.kernel_logits]\n",
        "\n",
        "\n",
        "def run_fednas_5epochs(\n",
        "    epochs=5,\n",
        "    local_epochs=1,\n",
        "    clients_per_round=10, # all clients participate each epoch\n",
        "    lr=1e-3, weight_decay=1e-4, batch_size=32,\n",
        "    beta_energy=0.02      # trade-off weight for energy in reward\n",
        "):\n",
        "    print(f\"DEVICE: {DEVICE}\")\n",
        "\n",
        "    # Build clients\n",
        "    clients = [Client(i, client_indices[i], batch_size=batch_size) for i in range(10)]\n",
        "    controller = RLController().to(DEVICE)\n",
        "    opt_ctrl = torch.optim.Adam(controller.parameters_list(), lr=5e-2)\n",
        "\n",
        "    global_rows = []   # one row per epoch\n",
        "    client_rows = []   # one row per client per epoch\n",
        "\n",
        "    def init_weights(m):\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "            if getattr(m, \"bias\", None) is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        if isinstance(m, nn.BatchNorm2d):\n",
        "            nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        print(\"\\n==============================\")\n",
        "        print(f\" Global Epoch {ep}/{epochs} (NAS sample + 1 FL round)\")\n",
        "        arch, logp = controller.sample_arch()\n",
        "        print(f\"  Sampled arch: depth={arch['depth']}, width={arch['width']}, kernel={arch['kernel']}\")\n",
        "\n",
        "        # fresh global model for this epoch's architecture\n",
        "        global_model = build_model(arch)\n",
        "        global_model.apply(init_weights)\n",
        "\n",
        "        # one FL round\n",
        "        selected = sorted(random.sample(range(10), clients_per_round))\n",
        "        client_payloads = []\n",
        "        total_local_energy_wh = 0.0\n",
        "        train_times_sec = []\n",
        "\n",
        "        print(\"  Client metrics:\")\n",
        "        for cid in selected:\n",
        "            sd, n, eWh, lacc, tsec = clients[cid].local_train(\n",
        "                global_model, epochs=local_epochs, lr=lr, weight_decay=weight_decay\n",
        "            )\n",
        "            client_payloads.append((sd, n, eWh, lacc, tsec))\n",
        "            total_local_energy_wh += eWh\n",
        "            train_times_sec.append(tsec)\n",
        "\n",
        "            print(f\"    â€¢ Client {cid:02d} | samples={n:5d} | local_acc={lacc*100:6.2f}% | \"\n",
        "                  f\"energy={eWh:.4f} Wh | time={tsec:.3f} s\")\n",
        "\n",
        "            client_rows.append({\n",
        "                \"epoch\": ep,\n",
        "                \"client_id\": cid,\n",
        "                \"samples\": n,\n",
        "                \"local_accuracy\": float(lacc),\n",
        "                \"local_energy_wh\": float(eWh),\n",
        "                \"train_time_sec\": float(tsec),\n",
        "                \"depth\": arch[\"depth\"],\n",
        "                \"width\": arch[\"width\"],\n",
        "                \"kernel\": arch[\"kernel\"],\n",
        "            })\n",
        "\n",
        "\n",
        "        agg_cc = CCTracker(name=f\"aggregation_ep{ep}\")\n",
        "        agg_cc.start()\n",
        "        global_model = fedavg(global_model, client_payloads)\n",
        "        agg_energy_wh = agg_cc.stop_wh()\n",
        "\n",
        "\n",
        "        global_acc = evaluate(global_model, full_test, batch_size=256)\n",
        "        total_energy_wh = total_local_energy_wh + agg_energy_wh\n",
        "\n",
        "\n",
        "        params_cnt = count_params(global_model)\n",
        "        flops_ps   = estimate_homocnn_flops_per_sample(global_model, input_hw=(32,32))\n",
        "\n",
        "\n",
        "        avg_tsec   = float(np.mean(train_times_sec)) if len(train_times_sec) else 0.0\n",
        "        max_tsec   = float(np.max(train_times_sec))  if len(train_times_sec) else 0.0\n",
        "\n",
        "        print(f\"  Aggregation energy: {agg_energy_wh:.6f} Wh\")\n",
        "        print(f\"  Model params: {params_cnt:,} | FLOPs/sample: {flops_ps:,}\")\n",
        "        print(f\"  Avg client train time: {avg_tsec:.3f}s | Max client train time: {max_tsec:.3f}s\")\n",
        "        print(f\"  â†’ GLOBAL after aggregation: Acc={global_acc*100:.2f}% | TotalEnergy={total_energy_wh:.4f} Wh\")\n",
        "\n",
        "        # REINFORCE update (reward: accuracy - beta*total_energy)\n",
        "        reward = float(global_acc) - float(beta_energy) * float(total_energy_wh)\n",
        "        opt_ctrl.zero_grad()\n",
        "        loss = -logp.to(DEVICE) * torch.tensor(reward, dtype=torch.float32, device=DEVICE)\n",
        "        loss.backward()\n",
        "        opt_ctrl.step()\n",
        "\n",
        "        global_rows.append({\n",
        "            \"epoch\": ep,\n",
        "            \"depth\": arch[\"depth\"],\n",
        "            \"width\": arch[\"width\"],\n",
        "            \"kernel\": arch[\"kernel\"],\n",
        "            \"params\": int(params_cnt),\n",
        "            \"flops_per_sample\": int(flops_ps),\n",
        "            \"avg_client_train_time_sec\": float(avg_tsec),\n",
        "            \"max_client_train_time_sec\": float(max_tsec),\n",
        "            \"global_accuracy\": float(global_acc),\n",
        "            \"total_local_energy_wh\": float(total_local_energy_wh),\n",
        "            \"aggregation_energy_wh\": float(agg_energy_wh),\n",
        "            \"total_energy_wh\": float(total_energy_wh),\n",
        "            \"reward\": float(reward),\n",
        "            \"k_selected\": int(len(selected)),\n",
        "        })\n",
        "\n",
        "\n",
        "    df_global = pd.DataFrame(global_rows)\n",
        "    df_clients = pd.DataFrame(client_rows)\n",
        "    df_global.to_csv(\"results_global.csv\", index=False)\n",
        "    df_clients.to_csv(\"results_clients.csv\", index=False)\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\" Per-epoch GLOBAL results:\")\n",
        "    for r in global_rows:\n",
        "        print(f\"  Epoch {r['epoch']}: Acc={r['global_accuracy']*100:.2f}%, \"\n",
        "              f\"Params={r['params']}, FLOPs={r['flops_per_sample']}, \"\n",
        "              f\"AvgT={r['avg_client_train_time_sec']:.3f}s, \"\n",
        "              f\"LocalEnergy={r['total_local_energy_wh']:.4f} Wh, \"\n",
        "              f\"AggEnergy={r['aggregation_energy_wh']:.6f} Wh, \"\n",
        "              f\"Total={r['total_energy_wh']:.4f} Wh\")\n",
        "\n",
        "    print(\"\\nðŸ“„ Saved CSVs: results_global.csv, results_clients.csv\")\n",
        "\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    from matplotlib import rcParams\n",
        "\n",
        "    rcParams.update({\n",
        "        \"font.family\": \"serif\",\n",
        "        \"font.serif\": [\"Times New Roman\", \"Times\", \"DejaVu Serif\"],\n",
        "        \"axes.linewidth\": 0.8,\n",
        "        \"axes.labelsize\": 11,\n",
        "        \"axes.titlesize\": 12,\n",
        "        \"xtick.labelsize\": 10,\n",
        "        \"ytick.labelsize\": 10,\n",
        "        \"legend.fontsize\": 10,\n",
        "        \"figure.dpi\": 300,\n",
        "    })\n",
        "\n",
        "    E = df_global[\"epoch\"].to_numpy()\n",
        "    acc = (df_global[\"global_accuracy\"].to_numpy() * 100.0)\n",
        "    params = df_global[\"params\"].to_numpy()\n",
        "    flops = df_global[\"flops_per_sample\"].to_numpy()\n",
        "    tavg = df_global[\"avg_client_train_time_sec\"].to_numpy()\n",
        "    ksel = df_global[\"k_selected\"].to_numpy()\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(5.2, 3.4))\n",
        "    plt.plot(params, acc, marker=\"o\")\n",
        "    for i in range(len(E)):\n",
        "        plt.annotate(f\"R{int(E[i])}\", (params[i], acc[i]),\n",
        "                     textcoords=\"offset points\", xytext=(5, 4), fontsize=8)\n",
        "    plt.xlabel(\"Model Parameters (count)\")\n",
        "    plt.ylabel(\"Test Accuracy (%)\")\n",
        "    plt.title(\"Experiment 4A â€” Accuracy vs Params (per round)\")\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(5.2, 3.4))\n",
        "    plt.plot(flops, acc, marker=\"o\")\n",
        "    for i in range(len(E)):\n",
        "        plt.annotate(f\"R{int(E[i])}\", (flops[i], acc[i]),\n",
        "                     textcoords=\"offset points\", xytext=(5, 4), fontsize=8)\n",
        "    plt.xlabel(\"Model FLOPs per Sample (forward pass)\")\n",
        "    plt.ylabel(\"Test Accuracy (%)\")\n",
        "    plt.title(\"Experiment 4B â€” Accuracy vs FLOPs (per round)\")\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(5.2, 3.4))\n",
        "    plt.plot(tavg, acc, marker=\"o\")\n",
        "    for i in range(len(E)):\n",
        "        plt.annotate(f\"R{int(E[i])} (k={int(ksel[i])})\", (tavg[i], acc[i]),\n",
        "                     textcoords=\"offset points\", xytext=(5, 4), fontsize=8)\n",
        "    plt.xlabel(\"Avg Client Training Time per Round (s)\")\n",
        "    plt.ylabel(\"Test Accuracy (%)\")\n",
        "    plt.title(\"Experiment 5C â€” Accuracy vs Client Training Time (per round)\")\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return df_global, df_clients\n",
        "\n",
        "\n",
        "df_global, df_clients = run_fednas_5epochs(\n",
        "    epochs=5,\n",
        "    local_epochs=1,\n",
        "    clients_per_round=10,\n",
        "    lr=0.001,\n",
        "    weight_decay=0.0001,\n",
        "    batch_size=32,\n",
        "    beta_energy=0.02\n",
        ")\n",
        "\n",
        "df_global.head(), df_clients.head()\n"
      ]
    }
  ]
}