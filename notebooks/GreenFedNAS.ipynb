{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os, random, copy, glob\n",
        "from typing import List, Dict, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "!pip install -qqq codecarbon\n",
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# =========================\n",
        "# Config\n",
        "# =========================\n",
        "DATA_CFG = {\n",
        "    \"img_size\": (32, 32, 3),\n",
        "    \"num_classes\": 10,\n",
        "}\n",
        "\n",
        "FL_CFG = {\n",
        "    \"num_clients\": 10,\n",
        "    \"rounds\": 5,\n",
        "    \"client_participation\": 0.8,\n",
        "    \"batch_size\": 64,\n",
        "    \"micro_epochs_min\": 1,\n",
        "    \"micro_epochs_max\": 3,\n",
        "    \"lr\": 1e-3,\n",
        "    \"candidates_per_client\": 5,\n",
        "    \"top_k\": 2,\n",
        "    \"dirichlet_alpha\": 0.5,\n",
        "    \"max_mutants\": 2,\n",
        "}\n",
        "\n",
        "AGG_CFG = {\"kd_T\": 2.0, \"kd_alpha\": 0.7}\n",
        "\n",
        "ENERGY_CFG = {\n",
        "    \"save_dir\": \"runs_energy\",\n",
        "    \"log_level\": \"warning\",\n",
        "}\n",
        "\n",
        "SEL_CFG = {\n",
        "    \"lambda_div\": 0.2,\n",
        "    \"epsilon_explore\": 0.2\n",
        "}\n",
        "\n",
        "os.makedirs(ENERGY_CFG[\"save_dir\"], exist_ok=True)\n",
        "\n",
        "# =========================\n",
        "# Data Loading\n",
        "# =========================\n",
        "def load_keras_data(dataset=\"cifar10\"):\n",
        "    if dataset.lower() == \"cifar10\":\n",
        "        (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported dataset\")\n",
        "\n",
        "    x_train = x_train.astype(\"float32\") / 255.0\n",
        "    x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    Xtr = torch.tensor(x_train).permute(0,3,1,2)  # N,H,W,C â†’ N,C,H,W\n",
        "    ytr = torch.tensor(y_train.squeeze(), dtype=torch.long)\n",
        "    Xte = torch.tensor(x_test).permute(0,3,1,2)\n",
        "    yte = torch.tensor(y_test.squeeze(), dtype=torch.long)\n",
        "    return Xtr, ytr, Xte, yte\n",
        "\n",
        "def make_loader(X, y, batch_size=64, shuffle=True):\n",
        "    return DataLoader(TensorDataset(X, y), batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "def dirichlet_partition(y: torch.Tensor, num_clients:int, alpha:float) -> List[np.ndarray]:\n",
        "    y_np = y.numpy()\n",
        "    classes = np.unique(y_np)\n",
        "    idx_per_class = [np.where(y_np == c)[0] for c in classes]\n",
        "    client_indices = [[] for _ in range(num_clients)]\n",
        "    for cls_idxs in idx_per_class:\n",
        "        np.random.shuffle(cls_idxs)\n",
        "        proportions = np.random.dirichlet([alpha]*num_clients)\n",
        "        cuts = (np.cumsum(proportions) * len(cls_idxs)).astype(int)[:-1]\n",
        "        splits = np.split(cls_idxs, cuts)\n",
        "        for i, split in enumerate(splits):\n",
        "            client_indices[i].extend(split.tolist())\n",
        "    for i in range(num_clients):\n",
        "        random.shuffle(client_indices[i])\n",
        "    return [np.array(ci, dtype=int) for ci in client_indices]\n",
        "\n",
        "# =========================\n",
        "# NAS Search Space & Model\n",
        "# =========================\n",
        "ACTS = {\"relu\": nn.ReLU, \"gelu\": nn.GELU, \"silu\": nn.SiLU}\n",
        "OPSET = [\n",
        "    {\"type\": \"conv3\", \"k\": 3, \"sep\": False, \"pool\": None},\n",
        "    {\"type\": \"conv5\", \"k\": 5, \"sep\": False, \"pool\": None},\n",
        "    {\"type\": \"sep3\",  \"k\": 3, \"sep\": True,  \"pool\": None},\n",
        "    {\"type\": \"max2\",  \"k\": 2, \"sep\": False, \"pool\": \"max\"},\n",
        "]\n",
        "WIDTHS = [8,16,32,64]\n",
        "DEPTHS = [2,3,4,5]\n",
        "ACT_CHOICES = [\"relu\",\"gelu\",\"silu\"]\n",
        "\n",
        "def random_arch():\n",
        "    depth = random.choice(DEPTHS)\n",
        "    act = random.choice(ACT_CHOICES)\n",
        "    chs = [random.choice(WIDTHS) for _ in range(depth)]\n",
        "    ops = [random.choice(OPSET) for _ in range(depth)]\n",
        "    return {\"depth\": depth, \"channels\": chs, \"ops\": ops, \"act\": act}\n",
        "\n",
        "class SepConv(nn.Module):\n",
        "    def __init__(self, c_in, c_out, k):\n",
        "        super().__init__()\n",
        "        pad = k//2\n",
        "        self.depth = nn.Conv2d(c_in, c_in, k, padding=pad, groups=c_in, bias=False)\n",
        "        self.point = nn.Conv2d(c_in, c_out, 1, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(c_out)\n",
        "    def forward(self, x):\n",
        "        x = self.depth(x)\n",
        "        x = self.point(x)\n",
        "        return self.bn(x)\n",
        "\n",
        "def make_block(c_in, c_out, op, act_name):\n",
        "    if op[\"pool\"]==\"max\":\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(c_in, c_out, 1, bias=False), nn.BatchNorm2d(c_out),\n",
        "            ACTS[act_name](), nn.MaxPool2d(op[\"k\"])\n",
        "        )\n",
        "    if op[\"sep\"]:\n",
        "        conv = SepConv(c_in, c_out, op[\"k\"])\n",
        "    else:\n",
        "        conv = nn.Conv2d(c_in, c_out, op[\"k\"], padding=op[\"k\"]//2, bias=False)\n",
        "    return nn.Sequential(conv, nn.BatchNorm2d(c_out), ACTS[act_name]())\n",
        "\n",
        "class SubNet(nn.Module):\n",
        "    def __init__(self, arch, num_classes):\n",
        "        super().__init__()\n",
        "        self.arch = copy.deepcopy(arch)\n",
        "        layers, c_in = [], 3\n",
        "        for i in range(arch[\"depth\"]):\n",
        "            c_out = arch[\"channels\"][i]\n",
        "            layers.append(make_block(c_in, c_out, arch[\"ops\"][i], arch[\"act\"]))\n",
        "            c_in = c_out\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.head = nn.Linear(c_in, num_classes)\n",
        "    def forward(self, x, return_feats=False):\n",
        "        x = self.features(x)\n",
        "        f = self.pool(x).view(x.size(0), -1)\n",
        "        logits = self.head(f)\n",
        "        return (logits,f) if return_feats else logits\n",
        "\n",
        "# =========================\n",
        "# Training / KD / Aggregation / Mutation\n",
        "# =========================\n",
        "def mgm_proxy(model: nn.Module, loader: DataLoader, num_batches:int=1) -> float:\n",
        "    model.train()\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    vals, seen = [], 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        xb.requires_grad_(True)\n",
        "        logits, feats = model(xb, return_feats=True)\n",
        "        loss = crit(logits, yb)\n",
        "        loss.backward(retain_graph=True)\n",
        "        G = feats @ feats.t()\n",
        "        score = G.mean().detach().item()\n",
        "        gnorm = sum((p.grad.data.norm(2).item() for p in model.parameters() if p.grad is not None), 0.0)\n",
        "        vals.append(score / (1.0 + gnorm))\n",
        "        model.zero_grad(set_to_none=True)\n",
        "        xb.requires_grad_(False)\n",
        "        seen += 1\n",
        "        if seen >= num_batches:\n",
        "            break\n",
        "    return float(np.mean(vals)) if vals else 0.0\n",
        "\n",
        "def train_epochs(model, loader, epochs=1, lr=1e-3):\n",
        "    model.train()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    for _ in range(epochs):\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            opt.zero_grad(); loss = crit(model(xb), yb)\n",
        "            loss.backward(); opt.step()\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total, loss_sum = 0, 0, 0.0\n",
        "    crit = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            logits = model(xb)\n",
        "            loss_sum += crit(logits, yb).item()\n",
        "            pred = logits.argmax(1)\n",
        "            correct += (pred == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "    return correct/total, loss_sum/total\n",
        "\n",
        "def kd_train(student, teacher_logits_fn, loader, epochs=1, lr=1e-3, T=2.0, alpha=0.7):\n",
        "    student.train()\n",
        "    opt = torch.optim.Adam(student.parameters(), lr=lr)\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    for _ in range(epochs):\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                tlogits = teacher_logits_fn(xb)\n",
        "            slogits = student(xb)\n",
        "            kd = F.kl_div(F.log_softmax(slogits/T, dim=1), F.softmax(tlogits/T, dim=1), reduction=\"batchmean\")*(T*T)\n",
        "            loss = alpha*kd + (1-alpha)*ce(slogits, yb)\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "\n",
        "def kd_from_hetero(global_model, teachers: List[nn.Module], loader, epochs=1, T=2.0, alpha=0.7, lr=1e-3):\n",
        "    def teacher_logits_fn(x):\n",
        "        outs = []\n",
        "        for t in teachers:\n",
        "            t.eval()\n",
        "            with torch.no_grad():\n",
        "                outs.append(t(x))\n",
        "        return torch.stack(outs, dim=0).mean(0)\n",
        "    kd_train(global_model, teacher_logits_fn, loader, epochs=epochs, lr=lr, T=T, alpha=alpha)\n",
        "\n",
        "def aligned_average(global_model: nn.Module, client_models: List[nn.Module]):\n",
        "    gstate = global_model.state_dict()\n",
        "    sums = {k: torch.zeros_like(v) for k, v in gstate.items()}\n",
        "    counts = {k: 0 for k in gstate.keys()}\n",
        "    for m in client_models:\n",
        "        s = m.state_dict()\n",
        "        for k, v in s.items():\n",
        "            if k in sums and v.shape == sums[k].shape:\n",
        "                sums[k] += v; counts[k] += 1\n",
        "    for k in gstate.keys():\n",
        "        if counts[k] > 0:\n",
        "            gstate[k] = sums[k]/counts[k]\n",
        "    global_model.load_state_dict(gstate, strict=False)\n",
        "\n",
        "def project_align_and_avg(global_model, client_models: List[nn.Module], loader=None, epochs=1, lr=1e-3):\n",
        "    if loader is not None:\n",
        "        kd_from_hetero(global_model, client_models, loader, epochs=epochs, T=2.0, alpha=0.7, lr=lr)\n",
        "\n",
        "def mutate_arch(arch):\n",
        "    a = copy.deepcopy(arch)\n",
        "    ch = random.choice([\"op\", \"width\", \"act\", \"depth\"])\n",
        "    if ch == \"op\":\n",
        "        i = random.randrange(a[\"depth\"]); a[\"ops\"][i] = random.choice(OPSET)\n",
        "    elif ch == \"width\":\n",
        "        i = random.randrange(a[\"depth\"]); a[\"channels\"][i] = random.choice(WIDTHS)\n",
        "    elif ch == \"act\":\n",
        "        a[\"act\"] = random.choice(ACT_CHOICES)\n",
        "    else:\n",
        "        if a[\"depth\"] < max(DEPTHS) and random.random() < 0.5:\n",
        "            a[\"depth\"] += 1; a[\"channels\"].append(random.choice(WIDTHS)); a[\"ops\"].append(random.choice(OPSET))\n",
        "        elif a[\"depth\"] > min(DEPTHS):\n",
        "            a[\"depth\"] -= 1; a[\"channels\"].pop(); a[\"ops\"].pop()\n",
        "    return a\n",
        "\n",
        "# =========================\n",
        "# CodeCarbon\n",
        "# =========================\n",
        "def start_tracker(name):\n",
        "    tracker = EmissionsTracker(\n",
        "        project_name=name,\n",
        "        output_dir=ENERGY_CFG[\"save_dir\"],\n",
        "        save_to_file=True,\n",
        "        measure_power_secs=1,\n",
        "        log_level=ENERGY_CFG[\"log_level\"],\n",
        "        emissions_endpoint=None\n",
        "    )\n",
        "    tracker.start()\n",
        "    return tracker\n",
        "\n",
        "def stop_tracker(tracker):\n",
        "    try:\n",
        "        tracker.stop()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def last_run_energy_Wh():\n",
        "    files = sorted(glob.glob(os.path.join(ENERGY_CFG[\"save_dir\"], \"*.csv\")), key=os.path.getmtime)\n",
        "    if not files:\n",
        "        return 0.0\n",
        "    try:\n",
        "        df = pd.read_csv(files[-1])\n",
        "        if \"energy_consumed\" in df.columns:\n",
        "            kwh = float(df[\"energy_consumed\"].iloc[-1])\n",
        "            return kwh * 1000.0  # Wh\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "# =========================\n",
        "# Diversity & Client Selection\n",
        "# =========================\n",
        "def arch_signature(arch):\n",
        "    depth = arch[\"depth\"]\n",
        "    mean_w = int(np.mean(arch[\"channels\"]))\n",
        "    act = arch[\"act\"]\n",
        "    ops = tuple(o[\"type\"] for o in arch[\"ops\"])\n",
        "    return (depth, mean_w, act, ops)\n",
        "\n",
        "def arch_distance(sig_a, sig_b):\n",
        "    if sig_a is None or sig_b is None:\n",
        "        return 0.0\n",
        "    d = 0.0\n",
        "    d += abs(sig_a[0] - sig_b[0]) / 3.0\n",
        "    d += abs(sig_a[1] - sig_b[1]) / 64.0\n",
        "    d += 0.5 * (0 if sig_a[2] == sig_b[2] else 1)\n",
        "    ops_a, ops_b = sig_a[3], sig_b[3]\n",
        "    L = max(len(ops_a), len(ops_b))\n",
        "    if L > 0:\n",
        "        match = sum(1 for i in range(min(len(ops_a), len(ops_b)))\n",
        "                    if ops_a[i] == ops_b[i])\n",
        "        d += (1 - match / L)\n",
        "    return d\n",
        "\n",
        "def energy_aware_select(client_hist: Dict[int, dict], m: int, num_clients: int) -> List[int]:\n",
        "    all_ids = list(range(num_clients))\n",
        "    selected, selected_sigs = [], []\n",
        "    known = [i for i in all_ids if i in client_hist]\n",
        "    unknown = [i for i in all_ids if i not in client_hist]\n",
        "\n",
        "    while len(selected) < m:\n",
        "        best_id, best_score = None, -1e9\n",
        "        if unknown and random.random() < SEL_CFG[\"epsilon_explore\"]:\n",
        "            pick = random.choice(unknown)\n",
        "            selected.append(pick); selected_sigs.append(None)\n",
        "            unknown.remove(pick)\n",
        "            continue\n",
        "\n",
        "        for cid in known:\n",
        "            if cid in selected:\n",
        "                continue\n",
        "            h = client_hist[cid]\n",
        "            acc = max(0.0, h.get(\"local_acc\", 0.0))\n",
        "            energy = max(1e-6, h.get(\"energy_Wh\", 0.0))\n",
        "            base = acc / energy\n",
        "            sig = arch_signature(h.get(\"arch\")) if h.get(\"arch\") else None\n",
        "            if selected_sigs:\n",
        "                div = np.mean([arch_distance(sig, s) for s in selected_sigs])\n",
        "            else:\n",
        "                div = 0.0\n",
        "            score = base + SEL_CFG[\"lambda_div\"] * div\n",
        "            if score > best_score:\n",
        "                best_score, best_id = score, cid\n",
        "\n",
        "        if best_id is not None:\n",
        "            selected.append(best_id)\n",
        "            s_sig = arch_signature(client_hist[best_id].get(\"arch\")) if client_hist[best_id].get(\"arch\") else None\n",
        "            selected_sigs.append(s_sig)\n",
        "            known.remove(best_id)\n",
        "        elif unknown:\n",
        "            pick = random.choice(unknown)\n",
        "            selected.append(pick); selected_sigs.append(None)\n",
        "            unknown.remove(pick)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    pool = [i for i in all_ids if i not in selected]\n",
        "    if len(selected) < m and pool:\n",
        "        selected += random.sample(pool, m - len(selected))\n",
        "    return sorted(selected)\n",
        "\n",
        "# =========================\n",
        "# Improved Experiment Loop\n",
        "# =========================\n",
        "def run_one_experiment(num_clients: int):\n",
        "    FL_CFG[\"num_clients\"] = num_clients\n",
        "\n",
        "    Xtr, ytr, Xte, yte = load_keras_data(\"cifar10\")\n",
        "    test_loader = make_loader(Xte, yte, batch_size=128, shuffle=False)\n",
        "\n",
        "    idxs = dirichlet_partition(ytr, num_clients, FL_CFG[\"dirichlet_alpha\"])\n",
        "    client_loaders = [make_loader(Xtr[i], ytr[i], batch_size=FL_CFG[\"batch_size\"], shuffle=True) for i in idxs]\n",
        "\n",
        "    global_arch = random_arch()\n",
        "    global_model = SubNet(global_arch, DATA_CFG[\"num_classes\"]).to(DEVICE)\n",
        "\n",
        "    client_hist = {}\n",
        "    client_rows, global_rows = [], []\n",
        "\n",
        "    print(f\"\\n=== Running experiment with {num_clients} clients ===\")\n",
        "\n",
        "    for rnd in range(1, FL_CFG[\"rounds\"] + 1):\n",
        "        print(f\"\\n--- Round {rnd} ---\")\n",
        "        m = max(1, int(FL_CFG[\"client_participation\"] * num_clients))\n",
        "        if rnd == 1:\n",
        "            selected = sorted(random.sample(range(num_clients), m))\n",
        "        else:\n",
        "            selected = energy_aware_select(client_hist, m, num_clients)\n",
        "\n",
        "        client_models = []\n",
        "        for cid in selected:\n",
        "            dl = client_loaders[cid]\n",
        "            trk = start_tracker(f\"client_{cid}_round_{rnd}\")\n",
        "\n",
        "            # KNAS candidates\n",
        "            candidates = [random_arch() for _ in range(FL_CFG[\"candidates_per_client\"])]\n",
        "            scored = [(mgm_proxy(SubNet(a, DATA_CFG[\"num_classes\"]).to(DEVICE), dl, num_batches=2), a) for a in candidates]\n",
        "            scored.sort(key=lambda x: x[0], reverse=True)\n",
        "            top_arches = [a for _, a in scored[:FL_CFG[\"top_k\"]]]\n",
        "\n",
        "            best_acc, best_model, best_arch = -1.0, None, None\n",
        "            for a in top_arches:\n",
        "                model = SubNet(a, DATA_CFG[\"num_classes\"]).to(DEVICE)\n",
        "                micro_epochs = random.randint(FL_CFG[\"micro_epochs_min\"], FL_CFG[\"micro_epochs_max\"])\n",
        "                train_epochs(model, dl, epochs=micro_epochs, lr=FL_CFG[\"lr\"])\n",
        "                acc, _ = evaluate(model, dl)\n",
        "                if acc > best_acc:\n",
        "                    best_acc, best_model, best_arch = acc, model, a\n",
        "\n",
        "            stop_tracker(trk)\n",
        "            energy_Wh = last_run_energy_Wh()\n",
        "\n",
        "            client_hist[cid] = {\n",
        "                \"local_acc\": float(best_acc),\n",
        "                \"energy_Wh\": float(energy_Wh),\n",
        "                \"arch\": best_arch\n",
        "            }\n",
        "            client_models.append(best_model)\n",
        "            print(f\"Client {cid}: acc={best_acc:.3f}, energy={energy_Wh:.3f}Wh\")\n",
        "\n",
        "        # Aggregation + KD\n",
        "        mutants = [SubNet(mutate_arch(random.choice([client_hist[cid][\"arch\"] for cid in selected])), DATA_CFG[\"num_classes\"]).to(DEVICE)\n",
        "                   for _ in range(FL_CFG[\"max_mutants\"])]\n",
        "        aligned_average(global_model, client_models + mutants)\n",
        "        project_align_and_avg(global_model, client_models + mutants, loader=test_loader, epochs=1, lr=FL_CFG[\"lr\"])\n",
        "\n",
        "        g_acc, g_loss = evaluate(global_model, test_loader)\n",
        "        print(f\"Global model test acc: {g_acc:.3f}\")\n",
        "\n",
        "        global_rows.append({\"round\": rnd, \"global_acc\": g_acc, \"global_loss\": g_loss})\n",
        "        for cid in selected:\n",
        "            client_rows.append({\"round\": rnd, \"client\": cid,\n",
        "                                \"local_acc\": client_hist[cid][\"local_acc\"],\n",
        "                                \"energy_Wh\": client_hist[cid][\"energy_Wh\"]})\n",
        "\n",
        "    print(\"\\n=== Experiment done ===\")\n",
        "    return client_rows, global_rows, client_hist\n",
        "\n",
        "def main():\n",
        "    client_settings = [3, 5]\n",
        "    for nc in client_settings:\n",
        "        run_one_experiment(nc)\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "tEmsJfCaIM3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FlqQ1Yc5Y46_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}